{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Python Programming: Naive Bayes",
      "provenance": [],
      "collapsed_sections": [
        "L2AZSQI69LWk",
        "EcX1UM0w22nz",
        "8dDIovl6-Bdz",
        "Oxk9l9YaWgYw",
        "hCf-nbGD24qA",
        "3zlzFxHy2-ky",
        "JwUCSxpP3AHd"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hazrakeruboO/DS-Colabs/blob/main/Copy_of_Python_Programming_Naive_Bayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tj0s0p4DeymK"
      },
      "source": [
        "<font color=\"green\">*To start working on this notebook, or any other notebook that we will use in the Moringa Data Science Course, we will need to save our own copy of it. We can do this by clicking File > Save a Copy in Drive. We will then be able to make edits to our own copy of this notebook.*</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSgQHdZS21iD"
      },
      "source": [
        "# Python Programming: Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2AZSQI69LWk"
      },
      "source": [
        "## Example 1: Gaussian Naive Bayes Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mw9OAPal9LCI"
      },
      "source": [
        "# Example 1\n",
        "# ---\n",
        "# This type of classifier makes the assumption of normal distribution \n",
        "# thus can be best used in cases when all our features are continuous.\n",
        "# ---\n",
        "# Question: Predict the species of flower using 4 different features.\n",
        "# ---\n",
        "# \n",
        "OUR CODE GOES HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27ozeIT2BoT4"
      },
      "source": [
        "# Load libraries and datasets to be used in this example\n",
        "#\n",
        "from sklearn import datasets\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsdY4zOdB2ky"
      },
      "source": [
        "# Loading our data from python datasets\n",
        "# \n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbScEK1FGA0p"
      },
      "source": [
        "# Splitting our data into a training set and a test set\n",
        "# \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=6) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PV2q7CeTB6P7"
      },
      "source": [
        "# Training our model\n",
        "# \n",
        "clf = GaussianNB()  \n",
        "model = clf.fit(X_train, y_train) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfovw-T6CJ0U"
      },
      "source": [
        "# Predicting our test predictors\n",
        "predicted = model.predict(X_test)\n",
        "print(np.mean(predicted == y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtGeQcvkB-s6"
      },
      "source": [
        "# Predicting a new observation\n",
        "new_observation = [[ 10,  3,  4,  0.4]]\n",
        "\n",
        "new_prediction = model.predict(new_observation)\n",
        "new_prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcX1UM0w22nz"
      },
      "source": [
        "## Example 2: Multinomial Naive Bayes Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bczlBDgf2wd6"
      },
      "source": [
        "# Example 2\n",
        "# ---\n",
        "# While working with the multinomial naive bayes classifier, the features are assumed to be multinomially distributed. \n",
        "# This would mean that this type of classifier is commonly used when we have discrete data (e.g. movie ratings 1 and 5).\n",
        "# Let us see how this works.\n",
        "# ----\n",
        "# Question: Build a model to predict whether an sms message is spam or not.\n",
        "# ---\n",
        "# Dataset url = http://bit.ly/SpamCollectionDataset\n",
        "# ---\n",
        "# \n",
        "OUR CODE GOES HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAsqH_uz3h_p"
      },
      "source": [
        "# Importing our libraries \n",
        "\n",
        "# Importing pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Importing numpy\n",
        "import numpy as np\n",
        "\n",
        "# We will also download and import nlkt which is a tokenizer. \n",
        "# This library will help us break (messages) into individual linguistic units i.e. words.\n",
        "#\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTtlH2q34L4K"
      },
      "source": [
        "# Loading and previewing our dataset\n",
        "# \n",
        "df = pd.read_csv('http://bit.ly/SpamCollectionDataset', sep='\\t',  header = None, names = ['label', 'message'])\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkCF6JuX4oQM"
      },
      "source": [
        "# Pre-processing\n",
        "# We will first emoving useless variance for our task at hand \n",
        "# \n",
        "\n",
        "# Converting the labels from strings to binary values for our classifier\n",
        "# \n",
        "df['label'] = df.label.map({'ham': 0, 'spam': 1})\n",
        "\n",
        "# Converting all characters in the message to lower case\n",
        "# \n",
        "df['message'] = df.message.map(lambda x: x.lower())\n",
        "\n",
        "# Removing any punctuation\n",
        "# \n",
        "df['message'] = df.message.str.replace('[^\\w\\s]', '')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ftbUUxi5UmL"
      },
      "source": [
        "# Pre-processing \n",
        "# Tokenizing the messages into into single words using nltk. \n",
        "\n",
        "# Applying the tokenization\n",
        "# \n",
        "df['message'] = df['message'].apply(nltk.word_tokenize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcxdFpHp5xae"
      },
      "source": [
        "# Fifth, we will perform some word stemming. \n",
        "# The idea of stemming is to normalize our text for all variations of words carry the same meaning, \n",
        "# regardless of the tense. One of the most popular stemming algorithms is the Porter Stemmer:\n",
        "# \n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        " \n",
        "df['message'] = df['message'].apply(lambda x: [stemmer.stem(y) for y in x])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ezXwg2e53v6"
      },
      "source": [
        "# Finally, we will transform the data into occurrences, \n",
        "# which will be the features that we will feed into our model\n",
        "# \n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# This converts the list of words into space-separated strings\n",
        "df['message'] = df['message'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "count_vect = CountVectorizer()\n",
        "counts = count_vect.fit_transform(df['message'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UysRCO4Y6MVI"
      },
      "source": [
        "# We could leave it as the simple word-count per message, but it is better to use Term Frequency Inverse Document Frequency, more known as tf-idf\n",
        "# \n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "transformer = TfidfTransformer().fit(counts)\n",
        "\n",
        "counts = transformer.transform(counts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Knhu0766PtJ"
      },
      "source": [
        "# Training the Model\n",
        "# Now that we have performed feature extraction from our data, it is time to build our model. \n",
        "# We will start by splitting our data into training and test sets\n",
        "# \n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(counts, df['label'], test_size=0.1, random_state=69)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjd6-wy16cmg"
      },
      "source": [
        "# Fitting our model \n",
        "# Then, all that we have to do is initialize the Naive Bayes Classifier and fit the data. \n",
        "# For text classification problems, the Multinomial Naive Bayes Classifier is well-suited\n",
        "# \n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "model = MultinomialNB().fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsxzsJuE6mBf"
      },
      "source": [
        "# Evaluating the Model\n",
        "# Once we have put together our classifier, we can evaluate its performance in the testing set\n",
        "# \n",
        "predicted = model.predict(X_test)\n",
        "print(np.mean(predicted == y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dDIovl6-Bdz"
      },
      "source": [
        "## Example 3: Bernoulli Naive Bayes Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bFEsnHL-MAc"
      },
      "source": [
        "# Example 3\n",
        "# ---\n",
        "# Question: It is rare to get a scenario where you have to use the Bernoulli Naive Bayes Classifier. \n",
        "# However, such a case would assume that all our features are binary, \n",
        "# that is they take only two values (e.g. a nominal categorical feature that has been one-hot encoded).\n",
        "# In the following example we will generate a dataset to demonstrate the use of this Classifier.\n",
        "# ---\n",
        "# \n",
        "OUR CODE GOES HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IiBTveYHdub"
      },
      "source": [
        "# Importing our libraries\n",
        "# \n",
        "import numpy as np\n",
        "from sklearn.naive_bayes import BernoulliNB"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QI14CBvKHk9h"
      },
      "source": [
        "# Creating binary features and target data\n",
        "# \n",
        "# Creating three binary features\n",
        "X = np.random.randint(2, size=(100, 3))\n",
        "\n",
        "# Creating a binary target vector\n",
        "y = np.random.randint(2, size=(100, 1)).ravel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDg6XbQ7HzpW"
      },
      "source": [
        "# Viewing first ten observations\n",
        "# \n",
        "X[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3D_lyUeH4VB"
      },
      "source": [
        "# Training our Bernoulli Naive Bayes Classifier\n",
        "# \n",
        "# Creating oour Bernoulli Naive Bayes object with prior probabilities of each class\n",
        "clf = BernoulliNB()\n",
        "\n",
        "# Train model\n",
        "model = clf.fit(X, y)\n",
        "\n",
        "# model score\n",
        "model.score(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oxk9l9YaWgYw"
      },
      "source": [
        "## <font color=\"green\">Challenge 1</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9VOjyqrWpgj"
      },
      "source": [
        "# Challenge 1\n",
        "# ---\n",
        "# Question: Build a model to determine whether a mushroom is edible.\n",
        "# ---\n",
        "# Dataset url = http://bit.ly/MushroomDataset\n",
        "# \n",
        "OUR CODE GOES HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCf-nbGD24qA"
      },
      "source": [
        "## <font color=\"green\">Challenge 2</font> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfhQpHhe286F"
      },
      "source": [
        "# Challenge 2\n",
        "# ---\n",
        "# Question: Given the following two datasets, build a model to determine whether a passenger survived or not.\n",
        "# ---\n",
        "# Train Dataset url = http://bit.ly/TitanicDatasetTrain\n",
        "# Test Dataset url = http://bit.ly/TitanicDatasetTest\n",
        "# ---\n",
        "# \n",
        "OUR CODE GOES HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zlzFxHy2-ky"
      },
      "source": [
        "## <font color=\"green\">Challenge 3</font> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7ExY54F2_m2"
      },
      "source": [
        "# Challenge 3\n",
        "# ---\n",
        "# Question: Build a model to classify a type of glass given the following dataset.\n",
        "# ---\n",
        "# Dataset url = http://bit.ly/GlassDatasetB\n",
        "# Dataset info:\n",
        "# Type of glass: (class) \n",
        "# -) 1 window glass (from vehicle or building) \n",
        "# -) 2 not window glass (containers, tableware, or headlamps)\n",
        "# ---\n",
        "# \n",
        "OUR CODE GOES HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwUCSxpP3AHd"
      },
      "source": [
        "## <font color=\"green\">Challenge 4</font> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Me_kp5ka3BTh"
      },
      "source": [
        "# Challenge 4\n",
        "# ---\n",
        "# Question: Build a classifier to help determine whether future patients do or do not have heart disease.\n",
        "# ---\n",
        "# Dataset url = http://bit.ly/HeartDatasetNB\n",
        "# \n",
        "OUR CODE GOES HERE"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}